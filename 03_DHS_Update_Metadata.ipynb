{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the survey metadata tables in the database\n",
    "\n",
    "This code is to be run after step 02, which generates FlatRecordSpec and FlatValuesSpec csv files.\n",
    "\n",
    "This code loads those files to the corresponding metadata tables in the database (currently `dhs_survey_specs.dhs_table_specs_flat` and `dhs_survey_specs.dhs_value_descs`). \n",
    "\n",
    "It checks the version of the files present as parsed CSVs and compares to what's present in the database in order to make sure that the database contains the latest info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/pg_conn.txt') as conn_details:\n",
    "    conn_str_psyco = conn_details.readline()\n",
    "    conn_str_sqlalchemy = conn_details.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/harry/anaconda3/envs/geodev_38/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DHS_To_Database.lib03_Update_Metadata import SurveyMetadataHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_SCHEMA = 'dhs_survey_specs'\n",
    "DATA_SCHEMA = 'dhs_data_tables'\n",
    "\n",
    "TABLESPEC_TABLENAME = 'dhs_table_specs_flat'\n",
    "VALUESPEC_TABLENAME = 'dhs_value_descs'\n",
    "SURVEYLIST_TABLENAME = 'dhs_survey_listing'\n",
    "\n",
    "TABLE_SPEC_TABLE = \".\".join((SPEC_SCHEMA, TABLESPEC_TABLENAME))\n",
    "VALUE_SPEC_TABLE = \".\".join((SPEC_SCHEMA, VALUESPEC_TABLENAME))\n",
    "SURVEYLIST_TABLE = \".\".join((SPEC_SCHEMA, SURVEYLIST_TABLENAME))\n",
    "\n",
    "STAGING_FOLDER = \"/mnt/c/Users/harry/OneDrive - Nexus365/Informal_Cities/DHS_Data_And_Prep/Staging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(conn_str_sqlalchemy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - refresh DB cache of available surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a separate, preliminary step, ensure that the cache we store of the available survey listing from the DHS API is up to date.\n",
    "\n",
    "Load the list of all surveys from the DHS API. Note that in the past, this has been paginated, necessitating multiple calls to load it all. At present this isn't the case. As at 2021-01-08 there are 444 rows returned, so if you note less than this then it's likely pagination has been re-implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surveyid</th>\n",
       "      <th>dhs_countrycode</th>\n",
       "      <th>surveyyear</th>\n",
       "      <th>surveytype</th>\n",
       "      <th>surveystatus</th>\n",
       "      <th>countryname</th>\n",
       "      <th>surveyyearlabel</th>\n",
       "      <th>surveynum</th>\n",
       "      <th>indicatordata</th>\n",
       "      <th>regionname</th>\n",
       "      <th>...</th>\n",
       "      <th>numberofhouseholds</th>\n",
       "      <th>universeofwomen</th>\n",
       "      <th>numberofwomen</th>\n",
       "      <th>minagewomen</th>\n",
       "      <th>maxagewomen</th>\n",
       "      <th>universeofmen</th>\n",
       "      <th>numberofmen</th>\n",
       "      <th>minagemen</th>\n",
       "      <th>maxagemen</th>\n",
       "      <th>numberoffacilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF2010OTH</td>\n",
       "      <td>AF</td>\n",
       "      <td>2010</td>\n",
       "      <td>OTH</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2010</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>South &amp; Southeast Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>22351.0</td>\n",
       "      <td>Ever Married Women</td>\n",
       "      <td>47848.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF2015DHS</td>\n",
       "      <td>AF</td>\n",
       "      <td>2015</td>\n",
       "      <td>DHS</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2015</td>\n",
       "      <td>471</td>\n",
       "      <td>1</td>\n",
       "      <td>South &amp; Southeast Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>24395.0</td>\n",
       "      <td>Ever Married Women</td>\n",
       "      <td>29461.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Ever Married Men</td>\n",
       "      <td>10760.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF2018SPA</td>\n",
       "      <td>AF</td>\n",
       "      <td>2018</td>\n",
       "      <td>SPA</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2018-19</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>South &amp; Southeast Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AF2021DHS</td>\n",
       "      <td>AF</td>\n",
       "      <td>2021</td>\n",
       "      <td>DHS</td>\n",
       "      <td>Ongoing</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2021</td>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>South &amp; Southeast Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>33120.0</td>\n",
       "      <td>Ever Married Women</td>\n",
       "      <td>37146.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Ever Married Men</td>\n",
       "      <td>13672.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL2008DHS</td>\n",
       "      <td>AL</td>\n",
       "      <td>2008</td>\n",
       "      <td>DHS</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2008-09</td>\n",
       "      <td>327</td>\n",
       "      <td>1</td>\n",
       "      <td>North Africa/West Asia/Europe</td>\n",
       "      <td>...</td>\n",
       "      <td>7999.0</td>\n",
       "      <td>All Women</td>\n",
       "      <td>7584.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>All Men</td>\n",
       "      <td>3013.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    surveyid dhs_countrycode  surveyyear surveytype surveystatus  countryname  \\\n",
       "0  AF2010OTH              AF        2010        OTH    Completed  Afghanistan   \n",
       "1  AF2015DHS              AF        2015        DHS    Completed  Afghanistan   \n",
       "2  AF2018SPA              AF        2018        SPA    Completed  Afghanistan   \n",
       "3  AF2021DHS              AF        2021        DHS      Ongoing  Afghanistan   \n",
       "4  AL2008DHS              AL        2008        DHS    Completed      Albania   \n",
       "\n",
       "  surveyyearlabel  surveynum  indicatordata                     regionname  \\\n",
       "0            2010        348              0         South & Southeast Asia   \n",
       "1            2015        471              1         South & Southeast Asia   \n",
       "2         2018-19        543              0         South & Southeast Asia   \n",
       "3            2021        568              0         South & Southeast Asia   \n",
       "4         2008-09        327              1  North Africa/West Asia/Europe   \n",
       "\n",
       "   ... numberofhouseholds     universeofwomen numberofwomen minagewomen  \\\n",
       "0  ...            22351.0  Ever Married Women       47848.0        12.0   \n",
       "1  ...            24395.0  Ever Married Women       29461.0        15.0   \n",
       "2  ...                NaN                 NaN           NaN         NaN   \n",
       "3  ...            33120.0  Ever Married Women       37146.0        15.0   \n",
       "4  ...             7999.0           All Women        7584.0        15.0   \n",
       "\n",
       "  maxagewomen     universeofmen numberofmen minagemen  maxagemen  \\\n",
       "0        49.0               NaN         NaN       NaN        NaN   \n",
       "1        49.0  Ever Married Men     10760.0      15.0       49.0   \n",
       "2         NaN               NaN         NaN       NaN        NaN   \n",
       "3        49.0  Ever Married Men     13672.0      15.0       49.0   \n",
       "4        49.0           All Men      3013.0      15.0       49.0   \n",
       "\n",
       "   numberoffacilities  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2               142.0  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_survey_listing = pd.read_csv('http://api.dhsprogram.com/rest/dhs/surveys?f=csv&surveyStatus=all&perpage=2000')\n",
    "api_survey_listing.columns = api_survey_listing.columns.str.lower()\n",
    "api_survey_listing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_survey_listing = pd.read_sql(f'SELECT * from {SURVEYLIST_TABLE}', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if there's any in the API results that aren't in our DB copy of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surveyid</th>\n",
       "      <th>dhs_countrycode</th>\n",
       "      <th>surveyyear</th>\n",
       "      <th>surveytype</th>\n",
       "      <th>surveystatus</th>\n",
       "      <th>countryname</th>\n",
       "      <th>surveyyearlabel</th>\n",
       "      <th>surveynum</th>\n",
       "      <th>indicatordata</th>\n",
       "      <th>regionname</th>\n",
       "      <th>...</th>\n",
       "      <th>numberofhouseholds</th>\n",
       "      <th>universeofwomen</th>\n",
       "      <th>numberofwomen</th>\n",
       "      <th>minagewomen</th>\n",
       "      <th>maxagewomen</th>\n",
       "      <th>universeofmen</th>\n",
       "      <th>numberofmen</th>\n",
       "      <th>minagemen</th>\n",
       "      <th>maxagemen</th>\n",
       "      <th>numberoffacilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [surveyid, dhs_countrycode, surveyyear, surveytype, surveystatus, countryname, surveyyearlabel, surveynum, indicatordata, regionname, subregionname, publicationdate, releasedate, surveycharacteristicids, footnotes, fieldworkstart, fieldworkend, implementingorg, numberofsamplepoints, numberofhouseholds, universeofwomen, numberofwomen, minagewomen, maxagewomen, universeofmen, numberofmen, minagemen, maxagemen, numberoffacilities]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_survey_listing[~api_survey_listing['surveynum'].isin(our_survey_listing['surveynum'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's more in the API than our copy of the table, just drop and reload the whole table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(api_survey_listing) > len(our_survey_listing):\n",
    "    api_survey_listing.to_sql(name=SURVEYLIST_TABLENAME, con=engine, schema=SPEC_SCHEMA, \n",
    "                     index=False, if_exists='replace', method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Update the database survey metadata tables\n",
    "\n",
    "The database has one metadata table which records all the survey data tables and what columns they have (per survey). This contains the values from the parsed \"FlatRecordSpec\" csv files.\n",
    "\n",
    "Another metadata table records all the values that all the survey data table columns can take, and (in the case of coded values) what the numeric values mean. This contains the values from the parsed \"FlatValuesSpec\" files.\n",
    "\n",
    "For every CSV FlatRecordSpec and FlatValuesSpec file, we need to check if it's in the DB table at all (and load if not). If it is already there, then check if the info in the latest CSV matches what's in the DB. (As e.g. a re-released survey may change the specified width of some columns, or add some new data tables equating to extra metadata rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_helper = SurveyMetadataHelper(conn_str=conn_str_sqlalchemy, table_spec_table=TABLESPEC_TABLENAME,\n",
    "                             value_spec_table=VALUESPEC_TABLENAME, spec_schema=SPEC_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a simple list of all the surveys that appear in the database metadata tables at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_table_svys = db_helper.get_existing_table_surveys()\n",
    "existing_value_svys = db_helper.get_existing_value_surveys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '100', '101', '105', '106', '109', '11', '110', '111', '112'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_table_svys['surveyid'].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a listing of all the up-to-date CSV table and value specification files we've parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_csv_dir = os.path.join(STAGING_FOLDER, 'parsed_specs')\n",
    "\n",
    "tbl_files = glob.glob(os.path.join(spec_csv_dir, '*.FlatRecordSpec.csv'))\n",
    "val_files = glob.glob(os.path.join(spec_csv_dir, '*.FlatValuesSpec.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each survey may have more than one file. This is normal if one is \"individual recode\" and another is \"men's recode\", but if there's some other reason then we need to investigate.\n",
    "\n",
    "Create a list of all the files for each (numerically-identified) survey id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_files_dict = SurveyMetadataHelper.build_spec_files_dict(tbl_files)\n",
    "val_files_dict = SurveyMetadataHelper.build_spec_files_dict(val_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the input files are as expected\n",
    "\n",
    "Run a few checks on the filenames of the parsed table spec files, to make sure they are for known data types (IR or MR), and whether there are multiple versions present for any surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_locs = set()\n",
    "multiple_versions_per_filetype = set() # TODO\n",
    "unknown_filetypes = set()\n",
    "multiple_files_per_filetype = set()\n",
    "\n",
    "for surveyid, dict_type_filenames in tbl_files_dict.items():\n",
    "    survey_loc = None\n",
    "    survey_version = {'ir':None, 'mr':None}\n",
    "    types_this_survey = set()\n",
    "    for filetype, fn_ver_locs in dict_type_filenames.items():\n",
    "        \n",
    "        # check we don't have anything except for IR and MR datasets\n",
    "        if filetype not in ['ir', 'mr']:\n",
    "            unknown_filetypes.add(surveyid)\n",
    "        \n",
    "        # check we don't have more than one IR or MR per survey\n",
    "        # if we do, then it's a new version\n",
    "        if len(fn_ver_locs)>1:\n",
    "            multiple_files_per_filetype.add(surveyid)\n",
    "        for fn, new_version, new_loc in fn_ver_locs:\n",
    "            # check for location being unique across IR and MR files\n",
    "            if survey_loc is None:\n",
    "                survey_loc = new_loc\n",
    "            elif survey_loc != new_loc:\n",
    "                multiple_locs.add(surveyid)\n",
    "        \n",
    "            # version string is often different between IR and MR files, that's ok, \n",
    "            # we want to check for multiple versions per filetype\n",
    "            if survey_version[filetype] is None:\n",
    "                survey_version[filetype] = new_version\n",
    "            elif survey_version[filetype] != new_version:\n",
    "                multiple_versions_per_filetype.add(surveyid)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if there are any surveys with filetypes other than IR/MR?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_filetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for any unexpected files**\n",
    "\n",
    "We expect that we may have both IR and MR files for a survey i.e. up to two tablespec files. If we have more than two files per (numeric) survey then it's either due to there being different location identifiers (as with survey 156 where they released a national dataset plus multiple regional ones); or to some different filetype being present other than IR/MR; or to there being multiple versions present of the same survey (e.g. if we've re-downloaded and are working in the same folder after an update was released). Check which surveys are affected by this. \n",
    "\n",
    "This shows all surveys with more than one file per filetype, could be due to multiple locations OR versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'580'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_files_per_filetype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows those truly with multiple versions per filetype at same location. If so then we will drop and reload the latest one only. (It's also quite common for the IR and MR files to have a different version. Don't worry about this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'580'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_versions_per_filetype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of one survey, number 156 (India) the download contains a main data file for India nationally (IAIR42), plus one for each state. (These break the convention by which the first two characters of the filename give the country code of the survey, because they are named for the state instead e.g. the file KEIR42 does not refer to Kenya but Kerala. This is one reason why we prepend and use the numeric survey id instead.) We do not need these state-wise files. If there's any like this, just move them out of the folder and re-run.\n",
    "\n",
    "Here we check that there are no other surveys affected by this other than 156:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we only want to parse and load one version per filetype (IR/MR) and per survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and load/reload column spec files as necessary\n",
    "\n",
    "Although we've made various checks already, we'll still re-check in the processing loop that there aren't any unexpected files, and we'll also check that the database is consistent for each survey e.g. that it does not already contain multiple versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_helper._is_dry_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tbl_svys = []\n",
    "drop_tbl_svys = []\n",
    "new_tbl_svys = []\n",
    "\n",
    "good_val_svys = []\n",
    "reload_val_svys = []\n",
    "new_val_svys = []\n",
    "               \n",
    "def report_cols_all_good(surveyid, file_type, file_name):\n",
    "    print(f\"Survey {surveyid} {file_type.upper()} spec file ({file_name}) is already completely loaded\")\n",
    "    good_tbl_svys.append(os.path.basename(file_name).split('.')[0])\n",
    "    \n",
    "for surveyid, dict_type_filenames in tbl_files_dict.items():\n",
    "    some_only_in_csv = False\n",
    "    some_only_in_db = False\n",
    "    \n",
    "    locset = set()\n",
    "    has_dupes = False\n",
    "    for file_type, fn_ver_locs in dict_type_filenames.items():\n",
    "        for _, _, loc in fn_ver_locs:\n",
    "            locset.add(loc)\n",
    "        if len(locset)>1:\n",
    "            warnings.warn(f\"\"\"\n",
    "            ****** WARNING MULTIPLE LOCATIONS FOUND FOR SURVEYID {surveyid} FILETYPE {file_type}. CANNOT DETERMINE WHICH TO LOAD *****\n",
    "            ****** PLEASE REMOVE SURPLUS FILES AND RERUN FOR THIS WHOLE SURVEY*****\n",
    "            \"\"\")\n",
    "            has_dupes = True\n",
    "    if has_dupes:\n",
    "        continue\n",
    "    # shortcut other checks, if nothing for this survey is in DB at all then we definitely need to load\n",
    "    need_to_load_all_data = surveyid not in existing_table_svys.values\n",
    "    if need_to_load_all_data: # no files or part files from this svy loaded\n",
    "        for file_type, fn_ver_locs in dict_type_filenames.items():\n",
    "            # the versions are strings such as '70' or '7A' where the first character \n",
    "            # represents the phase (fixed for a survey) and the second the actual version \n",
    "            # of this file. Anyway, natural sort order by version is ok.\n",
    "            latest = sorted(fn_ver_locs, key=lambda tup: tup[1])[-1]\n",
    "            file_name, file_ver, file_loc = latest\n",
    "            print(f\"No data for survey {surveyid} / {file_type} are present: \"+\n",
    "                  f\"loading latest data for {file_type.upper()} from {file_name}\")\n",
    "            if len(fn_ver_locs) > 1: print(f\"   ({len(fn_ver_locs)} versions were available, loading {file_ver}\")\n",
    "            db_helper.load_new_metadata_file(file_name)\n",
    "        continue\n",
    "    \n",
    "    # metadata for this survey are already present. That doesn't necessarily mean we don't need to \n",
    "    # load or reload this file. Check.\n",
    "    for file_type, fn_ver_locs in dict_type_filenames.items():\n",
    "        latest = sorted(fn_ver_locs, key=lambda tup: tup[1])[-1]\n",
    "        file_name, file_ver, file_loc = latest\n",
    "        \n",
    "        # Check for the scenario where IR data are in the DB but MR data aren't, etc\n",
    "        if not db_helper.get_any_in_db_cols(surveyid, file_type):\n",
    "            print(f\"{file_type.upper()} data for survey {surveyid} are not present: loading from {file_name}\")\n",
    "            if len(fn_ver_locs) > 1: print(f\"    (of {len(fn_ver_locs)} available versions)\")\n",
    "            db_helper.load_new_metadata_file(file_name)\n",
    "            continue\n",
    "            \n",
    "        db_ver, db_has_single_ver = db_helper.get_db_survey_version_cols(surveyid, file_type)\n",
    "        if not db_has_single_ver:\n",
    "            db_helper.drop_and_reload(file_name, \n",
    "                                      msg=\"Multiple versions found in DB, dropping all and loading latest\")\n",
    "        \n",
    "        elif file_ver > db_ver: # version is alphanumeric but lexical comparison works\n",
    "            # this needs to be done for the data tables too\n",
    "            db_helper.drop_and_reload(file_name, \n",
    "                                      msg=f\"Newer version found - DB is version {db_ver} \"+\n",
    "                                      f\"and file is version {file_ver}\")\n",
    "        else:\n",
    "            # check that the same rows are present in the CSV and DB\n",
    "            file_cols_data = pd.read_csv(file_name)\n",
    "            db_cols_data = db_helper.get_tablespec_rows_for_svy_filetype(surveyid, file_type)\n",
    "            # do an outer join to find items present in files but not DB and vice versa\n",
    "            merged_cols = file_cols_data.merge(db_cols_data, how='outer',\n",
    "                                      left_on=['Name','RecordName','Label','Len'],\n",
    "                                      right_on=['name','recordname', 'label', 'len'],\n",
    "                                      suffixes=('_new', '_exist'),\n",
    "                                      indicator=True)\n",
    "            cols_all_good = len(merged_cols[merged_cols['_merge']!='both']) == 0\n",
    "            if not cols_all_good:\n",
    "                # Most likely reason for having new columns info is that we've added men's recode data,\n",
    "                # but that won't be the case here as the check included filetype.\n",
    "                # Reasons to be here would be it's a re-issued survey (same id number) and has added \n",
    "                # different columns (in which case there will be some left-only and no right-only ones), \n",
    "                # and/or that some column labels have changed (e.g. prefix of NA been added).\n",
    "                some_only_in_db = len(merged_cols[merged_cols['_merge'] == 'right_only']) > 0\n",
    "                some_only_in_csv = len(merged_cols[merged_cols['_merge'] == 'left_only']) > 0\n",
    "                if some_only_in_csv and some_only_in_db:\n",
    "                    # Not worth the effort to investigate: just drop everything from this survey and reload.\n",
    "                    db_helper.drop_and_reload(file_name, \n",
    "                                              msg=\"Rows mismatch between file and DB\")\n",
    "                elif some_only_in_csv:\n",
    "                    # Not worth the effort to investigate: just drop everything from this survey and reload.\n",
    "                    db_helper.drop_and_reload(file_name, \n",
    "                                              msg=\"Rows present in file but not in DB\")\n",
    "                elif some_only_in_db:\n",
    "                    # Not worth the effort to investigate: just drop everything from this survey and reload.\n",
    "                    db_helper.drop_and_reload(file_name, \n",
    "                                              msg=\"Rows present in DB but not in file\")\n",
    "            else:\n",
    "                report_cols_all_good(surveyid, file_type, file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and load/reload value spec files as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_tbl_svys = []\n",
    "drop_tbl_svys = []\n",
    "new_tbl_svys = []\n",
    "\n",
    "good_val_svys = []\n",
    "reload_val_svys = []\n",
    "new_val_svys = []\n",
    "\n",
    "\n",
    "def report_vals_all_good(surveyid, file_type, file_name):\n",
    "    print(f\"Survey {surveyid} {file_type.upper()} values file ({file_name}) is already completely loaded\")\n",
    "    good_val_svys.append(os.path.basename(file_name).split('.')[0])\n",
    "    \n",
    "for surveyid, dict_type_filenames in val_files_dict.items():\n",
    "    #if not surveyid.startswith('1'):\n",
    "    #    continue\n",
    "    some_only_in_csv = False\n",
    "    some_only_in_db = False\n",
    "    #if surveyid != '16':\n",
    "     #   continue\n",
    "    \n",
    "    locset = set()\n",
    "    has_dupes = False\n",
    "    for file_type, fn_ver_locs in dict_type_filenames.items():\n",
    "        for _, _, loc in fn_ver_locs:\n",
    "            locset.add(loc)\n",
    "        if len(locset)>1:\n",
    "            print(f\"\"\"\n",
    "            ****** WARNING MULTIPLE LOCATIONS FOUND FOR SURVEYID {surveyid} FILETYPE {file_type}. CANNOT DETERMINE WHICH TO LOAD *****\n",
    "            ****** PLEASE REMOVE SURPLUS FILES AND RERUN FOR THIS WHOLE SURVEY*****\n",
    "            \"\"\")\n",
    "            has_dupes = True\n",
    "    if has_dupes:\n",
    "        continue\n",
    "        \n",
    "    # shortcut other checks, if nothing for this survey is in DB at all then we definitely need to load\n",
    "    need_to_load_all_data = surveyid not in existing_value_svys.values\n",
    "    if need_to_load_all_data: # no files or part files from this svy loaded\n",
    "        for file_type, fn_ver_locs in dict_type_filenames.items():\n",
    "            # the versions are strings such as '70' or '7A' where the first character \n",
    "            # represents the phase (fixed for a survey) and the second the actual version \n",
    "            # of this file. Anyway, natural sort order by version is ok.\n",
    "            latest = sorted(fn_ver_locs, key=lambda tup: tup[1])[-1]\n",
    "            file_name, file_ver, file_loc = latest\n",
    "            print(f\"No data for survey {surveyid} / {file_type} are present: \"+\n",
    "                  f\"loading latest data for {file_type.upper()} from {file_name}\")\n",
    "            if len(fn_ver_locs) > 1: print(f\"   ({len(fn_ver_locs)} versions were available, loading {file_ver}\")\n",
    "            db_helper.load_new_metadata_file(file_name)\n",
    "        continue\n",
    "    for file_type, fn_ver_locs in dict_type_filenames.items():\n",
    "        latest = sorted(fn_ver_locs, key=lambda tup: tup[1])[-1]\n",
    "        file_name, file_ver, file_loc = latest\n",
    "        \n",
    "        # Check for the scenario where IR data are in the DB but MR data aren't, etc\n",
    "        if not db_helper.get_any_in_db_vals(surveyid, file_type):\n",
    "            print(f\"{file_type.upper()} value-spec data for survey {surveyid} are not present: loading from {file_name}\")\n",
    "            if len(fn_ver_locs) > 1: print(f\"    (of {len(fn_ver_locs)} available versions)\")\n",
    "            db_helper.load_new_metadata_file(file_name)\n",
    "            continue\n",
    "            \n",
    "        db_ver, db_has_single_ver = db_helper.get_db_survey_version_vals(surveyid, file_type)\n",
    "        if not db_has_single_ver:\n",
    "            db_helper.drop_and_reload(file_name,\n",
    "                            msg=\"Multiple versions found in DB, dropping all and loading latest\")\n",
    "        elif file_ver > db_ver:\n",
    "            # this needs to be done for the data tables too\n",
    "            db_helper.drop_and_reload(file_name,\n",
    "                            msg=f\"Newer version found - DB is version {db_ver} \"+\n",
    "                                      \"and file is version {file_ver}\")\n",
    "        else:\n",
    "            # check that the same rows are present in the CSV and DB\n",
    "            file_vals_data = pd.read_csv(file_name)\n",
    "            db_vals_data = db_helper.get_valuespec_rows_for_svy_filetype(surveyid, file_type)\n",
    "            # do an outer join to find items present in files but not DB and vice versa\n",
    "            merged_vals = file_vals_data.merge(db_vals_data, how='outer',\n",
    "                                      left_on=['Name','Value','ValueDesc','ValueType'],\n",
    "                                      right_on=['col_name','value', 'value_desc', 'value_type'],\n",
    "                                      suffixes=('_new', '_exist'),\n",
    "                                      indicator=True)\n",
    "            vals_all_good = len(merged_vals[merged_vals['_merge']!='both']) == 0\n",
    "            if not vals_all_good:\n",
    "                # Most likely reason is that we now have men's recode data as well, but did not previously, \n",
    "                # but we have checked for that already.\n",
    "                # Other reasons would be it's a re-issued survey (same id number) and has added different columns \n",
    "                # (in which case there will be some left-only and no right-only ones), and/or that some column \n",
    "                # labels have changed (e.g. prefix of NA been added).\n",
    "                some_only_in_db = len(merged_vals[merged_vals['_merge'] == 'right_only']) > 0\n",
    "                some_only_in_csv = len(merged_vals[merged_vals['_merge'] == 'left_only']) > 0\n",
    "                if some_only_in_csv and some_only_in_db:\n",
    "                    # Not worth the effort to investigate: just drop everything from this survey and reload.\n",
    "                    db_helper.drop_and_reload(file_name, \n",
    "                                              msg=\"Rows mismatch between file and DB\")\n",
    "                elif some_only_in_csv:\n",
    "                    # Not worth the effort to investigate: just drop everything from this survey and reload.\n",
    "                    db_helper.drop_and_reload(file_name, \n",
    "                                              msg=\"Rows present in file but not in DB\")\n",
    "                elif some_only_in_db:\n",
    "                    # Not worth the effort to investigate: just drop everything from this survey and reload.\n",
    "                    db_helper.drop_and_reload(file_name, \n",
    "                                              msg=\"Rows present in DB but not in file\")\n",
    "                    \n",
    "            else:\n",
    "                #pass\n",
    "                # TODO check 475\n",
    "                report_vals_all_good(surveyid, file_type, file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting errors\n",
    "\n",
    "Before this code was implemented, a survey occasionally got loaded twice to the column/value descriptions tables .\n",
    "\n",
    "#### Value descriptions\n",
    "\n",
    "Find duplicate rows in the value descriptions table, according to the surveyid, column name and value\n",
    "```sql \n",
    "SELECT count(*) as n_occ, surveyid, col_name, value \n",
    "FROM dhs_survey_specs.dhs_value_descs \n",
    "GROUP BY surveyid, col_name, value\n",
    "HAVING count(*)>1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_values = pd.read_sql('''SELECT count(*) as n_occ, surveyid, col_name, value \n",
    "FROM dhs_survey_specs.dhs_value_descs \n",
    "GROUP BY surveyid, col_name, value\n",
    "HAVING count(*)>1''', con=engine)\n",
    "duplicate_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to quite often be a few duplicates in weird calendar fields that I have not properly looked into. \n",
    "\n",
    "Consider there to be a problem if there are more than 1000 duplicates per survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows=200\n",
    "n_dupes = duplicate_values.groupby(['surveyid']).size().reset_index(name='counts')\n",
    "dodgy_vals = n_dupes[n_dupes['counts']>1000]\n",
    "repeat_val_load_str = \",\".join([\"'\"+str(s)+\"'\" for s in dodgy_vals['surveyid']])\n",
    "repeat_val_load_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE FROM dhs_survey_specs.dhs_value_descs WHERE surveyid in ('5','48','49','50','51','52','53','473','474','475','476','477','478','481','483','484','485','487','489','490','491','493','497','499','500','503','504','505','509','510','511','512','514','515','516','517','521','522','523','526','527','528','529','534');\n"
     ]
    }
   ],
   "source": [
    "delete_str = f\"DELETE FROM dhs_survey_specs.dhs_value_descs WHERE surveyid in ({repeat_val_load_str});\"\n",
    "print(delete_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to delete the misloaded data, then reload them using the code in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_occ</th>\n",
       "      <th>surveyid</th>\n",
       "      <th>recordname</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [n_occ, surveyid, recordname, name]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_cols = pd.read_sql('''SELECT count(*) as n_occ, surveyid, recordname, name\n",
    "FROM dhs_survey_specs.dhs_table_specs_flat \n",
    "GROUP BY surveyid, recordname, name\n",
    "HAVING count(*)>1''', con=engine)\n",
    "duplicate_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_cols['surveyid'].unique()\n",
    "\n",
    "pd.options.display.max_rows=200\n",
    "n_dupe_cols = duplicate_cols.groupby(['surveyid']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surveyid</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [surveyid, counts]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dupe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dodgy_cols = n_dupe_cols[n_dupe_cols['counts']>50]\n",
    "repeat_col_load_str = \",\".join([\"'\"+str(s)+\"'\" for s in dodgy_cols['surveyid']])\n",
    "dodgy_col_surveys=list(dodgy_cols['surveyid'])\n",
    "repeat_col_load_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE FROM dhs_survey_specs.dhs_table_specs_flat WHERE surveyid in ();\n"
     ]
    }
   ],
   "source": [
    "delete_str = f\"DELETE FROM dhs_survey_specs.dhs_table_specs_flat WHERE surveyid in ({repeat_col_load_str});\"\n",
    "print(delete_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
